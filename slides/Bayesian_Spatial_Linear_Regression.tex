%\documentclass[hyperref={pdfpagelabels=false},compress]{beamer}

% \titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.pdf}}

\input{frontStuff.tex}

\title[]{Bayesian Linear Models}
\input{authors.tex}

\begin{document}

\maketitle


\begin{frame}{Linear Regression}

\begin{itemize}\setlength{\itemsep}{0.5cm}
\item Linear regression is, perhaps, \emph{the} most widely used statistical modeling tool.

\item It addresses the following question: How does a quantity of primary interest, $y$, vary as (depend upon) another quantity, or set of quantities, $x$?  

\item The quantity $y$ is called the \emph{response} or \emph{outcome variable}. Some people simply refer to it as the \emph{dependent variable}.

\item The variable(s) $x$ are called \emph{explanatory variables}, \emph{covariates} or simply \emph{independent variables}.

\item In general, we are interested in the conditional distribution of $y$, given $x$, parametrized as $p(y\given \theta, x)$.
\end{itemize}

\end{frame}

\begin{frame}
 
\begin{itemize}\setlength{\itemsep}{0.4cm}
 \item Typically, we have a set of \emph{units} or \emph{experimental subjects} $i=1,2,\ldots,n$.

 \item For each of these units we have measured an outcome $y_i$ and a set of explanatory variables $\bx_i^{\top} = (1, x_{i1}, x_{i2}, \ldots, x_{ip})$. 

 \item The first element of $\bx_i^{\top}$ is often taken as $1$ to signify the presence of an ``intercept''. 
 
 \item We collect the outcome and explanatory variables into an $n\times 1$ vector and an $n\times (p+1)$ matrix:
\begin{align*}
 \by = \begin{pmatrix} 
	y_1 \\
        y_2 \\
	\vdots \\
	y_n
       \end{pmatrix};\quad \bX = \begin{bmatrix}
			1 & x_{11} & x_{12} & \ldots & x_{1p} \\
			1 & x_{21} & x_{22} & \ldots & x_{2p} \\
			\vdots & \vdots & \vdots & \vdots & \vdots \\
			1 & x_{n1} & x_{n2} & \ldots & x_{np}
                       \end{bmatrix} = \begin{pmatrix} 
	\bx_1^{\top} \\
        \bx_2^{\top} \\
	\vdots \\
	\bx_n^{\top}
       \end{pmatrix}\; . 
\end{align*}

\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}
 \item The linear model is the most fundamental of all serious statistical models underpinning:
\vspace*{0.5cm}
	\begin{itemize}\setlength{\itemsep}{0.4cm}
	\item ANOVA: $y_i$ is continuous, $x_{ij}$'s are \emph{all} categorical
	\item REGRESSION: $y_i$ is continuous, $x_{ij}$'s are continuous
	\item ANCOVA: $y_i$ is continuous, $x_{ij}$'s are continuous for some $j$ and categorical for others. 
	\end{itemize}
\end{itemize} 
\end{frame}

\begin{frame}{Conjugate Bayesian Linear Regression}
 
\begin{itemize}\setlength{\itemsep}{0.4cm}
 \item \vskip-2mm A conjugate Bayesian linear model is given by:
\begin{align*}
& y_i\given \bbeta,\sigma^2, \bx_i \stackrel{ind}{\sim} N(\mu_i,\sigma^2);\quad i=1,2,\ldots,n\;;\\
& \mu_i = \beta_0 + \beta_1x_{i1} + \cdots + \beta_{p} x_{ip} = \bx_i^{\top}\bbeta\;;\quad \bbeta = (\beta_0,\beta_1,\ldots,\beta_{p})^{\top}\;; \\
& \bbeta \given \sigma^2 \sim N(\bmu_{\beta}, \sigma^2 \bV_{\beta})\;;\quad  \sigma^2 \sim IG(a,b)\; .
\end{align*}

\item Unknown parameters include the regression parameters and the variance, i.e. $\btheta = \{\bbeta, \sigma^2\}$. 

\item We assume $\bX$ is observed without error and all inference is conditional on $\bX$.

\item The above model is often written in terms of the posterior density $p(\btheta \given \by) \propto p(\btheta, \by)$:
\begin{align*}
 IG(\sigma^2\given a,b) \times N(\bbeta \given \bmu_{\beta}, \sigma^2 \bV_{\beta}) \times \prod_{i=1}^n N(y_i \given \bx_i^{\top}\bbeta, \sigma^2)\;.
\end{align*}

\end{itemize}

\end{frame}


\begin{frame}{Conjugate Bayesian (General) Linear Regression}
 
\begin{itemize}\setlength{\itemsep}{0.1cm}
 \item \vskip-2mm A more general conjugate Bayesian linear model is given by:
\begin{align*}
& \by \given \bbeta,\sigma^2, \bX \sim N(\bX\bbeta, \sigma^2 \bV_y)\\
& \bbeta \given \sigma^2 \sim N(\bmu_{\beta}, \sigma^2 \bV_{\beta})\;;\\
& \sigma^2 \sim IG(a,b)\; .
\end{align*}

\item $\bV_y$, $\bV_{\beta}$ and $\bmu_{\beta}$ are assumed fixed.

\item Unknown parameters include the regression parameters and the variance, i.e. $\btheta = \{\bbeta, \sigma^2\}$. 

\item We assume $\bX$ is observed without error and all inference is conditional on $\bX$.

\item The posterior density $p(\btheta \given \by) \propto p(\btheta, \by)$:
\begin{align*}
 IG(\sigma^2\given a,b) \times N(\bbeta \given \bmu_{\beta}, \sigma^2 \bV_{\beta}) \times N(\by \given \bX\bbeta, \sigma^2 \bV_{y})
\end{align*}

\item The model on the previous slide is a special case with $\bV_y = \bI_n$ ($n\times n$ identity matrix).
\end{itemize}

\end{frame}

\begin{frame}{Conjugate Bayesian (General) Linear Regression}
 
\begin{itemize}\setlength{\itemsep}{0.1cm}
 \item The joint posterior density can be written as
\begin{align*}
 p(\bbeta,\sigma^2 \given \by) &\propto
 \begin{array}{ccc}
  \underbrace{IG(\sigma^2\given a^*, b^*)} & \times & \underbrace{N\left(\bbeta \given \bM\bm, \sigma^2 \bM\right)}\\
  p(\sigma^2\given \by) & & p(\bbeta\given \sigma^2, \by)
 \end{array}\; , 
\end{align*}
where 
\begin{align*}
& a^* = a + \frac{n}{2}\;;\quad b^* = b + \frac{1}{2}\left(\bmu_{\beta}^{\top}\bV_{\beta}^{-1}\bmu_{\beta} + \by^{\top}\bV_y^{-1}\by - \bm^{\top}\bM\bm\right)\;; \\ 
& \bm = \bV_{\beta}^{-1}\bmu_{\beta} + \bX^{\top}\bV_y^{-1}\by\;;\quad \bM^{-1} = \bV_{\beta}^{-1} + \bX^{\top} \bV_y^{-1} \bX\; .   
\end{align*}

\item Exact posterior sampling from $p(\bbeta, \sigma^2 \given \by)$ will automatically yield samples from $p(\bbeta\given \by)$ and $p(\sigma^2\given \by)$.

\item For each $i=1,2,\ldots,N$ do the following: 
 \begin{enumerate}
  \item Draw $\displaystyle \sigma^2_{(i)} \sim IG(a^*,b^*)$
  \item Draw $\displaystyle \beta_{(i)} \sim N\left(\bM\bm, \sigma^2_{(i)}\bM\right)$
 \end{enumerate}
 
 \item The above is sometimes referred to as \emph{composition sampling}.
\end{itemize}

\end{frame}

\begin{frame}{Exact sampling from joint posterior distributions}

\begin{itemize}\setlength{\itemsep}{0.25cm}
\item Suppose we wish to draw samples from a joint posterior:
\[
 p(\theta_1,\theta_2\given \by) = p(\theta_1\given \by) \times p(\theta_2\given \theta_1, \by)\; .
\]

\item In conjugate models, it is often easy to draw samples from $p(\theta_1\given \by)$ and from $p(\theta_2\given \theta_1,\by)$. 

\item We can draw $N$ samples from $p(\theta_1,\theta_2\given \by)$ as follows.

\item For each $i=1,2,\ldots,N$ do the following: 
 \begin{enumerate}\setlength{\itemsep}{0cm}
  \item Draw $\displaystyle \theta_{1(i)} \sim p(\theta_1\given \by)$
  \item Draw $\displaystyle \theta_{2(i)} \sim p(\theta_2\given \theta_1,\by)$
 \end{enumerate}

\item Remarkably, the $\theta_{2(i)}$'s drawn above have marginal distribution $p(\theta_2\given \by)$ (see, Gelfand and Smith 1990). %because: 
%\vskip -9mm \begin{align*} P(\theta_2\leq u \given y) &= \mbox{E}_{\theta_2|y}\left[1(\theta_2\leq u)\right] = \mbox{E}_{\theta_1|y}\left\{\mbox{E}_{\theta_2|\theta_1, y}\left[1(\theta_2\leq u)\right]\right\} \\
%  &\approx \frac{1}{N} \sum_{i=1}^N \mbox{E}_{\theta_2|\theta_{1(i)}, y}\left[1(\theta_2\leq u)\right] \approx  \frac{1}{N} \sum_{i=1}^N 1(\theta_{2(i)}\leq u)\; .
%\end{align*}

\item ``Automatic Marginalization'' we draw samples $p(\theta_1,\theta_2\given \by)$ and automatically get samples from $p(\theta_1\given \by)$ and $p(\theta_2\given \by)$.

\end{itemize}

\end{frame}

\begin{frame}{Bayesian predictions from linear regression}
 
 \begin{itemize}\setlength{\itemsep}{0.4cm}
%   \item Consider first the Bayesian linear model with $V_y = I_n$:
%   \[
%    IG(\sigma^2\given a,b) \times N(\beta \given \mu_{\beta}, \sigma^2 V_{\beta}) \times N(y \given X\beta, \sigma^2I_n)\;.
%   \]
  \item Let $\tilde{\by}$ denote an $m\times 1$ vector of outcomes we seek to predict based upon predictors $\tilde{\bX}$.
  \item We seek the posterior predictive density:
\[
 p(\tilde{\by}\given \by) = \int p(\tilde{\by}\given \btheta, \by) p(\btheta\given \by) \mbox{d}\btheta .
\]
  \item Posterior predictive inference: sample from $p(\tilde{\by}\given \by)$.
  \item For each $i=1,2,\ldots,N$ do the following: 
 \begin{enumerate}\setlength{\itemsep}{0.25cm}
  \item Draw $\displaystyle \btheta_{(i)} \sim p(\btheta\given \by)$
  \item Draw $\displaystyle \tilde{\by}_{(i)} \sim p(\tilde{\by}\given \btheta_{(i)},\by)$
 \end{enumerate}
  
 \end{itemize}

\end{frame}

\begin{frame}{Bayesian predictions from linear regression (cont'd)}
 
 \begin{itemize}\setlength{\itemsep}{0.cm}
  \item \vskip -2mm For legitimate probabilistic predictions (forecasting), the conditional distribution $p(\tilde{\by}\given \btheta, \by)$ must be well-defined.
  
  \item For example, consider the case with $\bV_y = \bI_n$. Specify the linear model:
\begin{align*}
 \begin{bmatrix} \by \\ \tilde{\by} \end{bmatrix} = \begin{bmatrix} \bX \\ \tilde{\bX} \end{bmatrix}\bbeta + \begin{bmatrix} \bepsilon \\ \tilde{\bepsilon} \end{bmatrix}\;;\quad \begin{bmatrix} \bepsilon \\ \tilde{\bepsilon} \end{bmatrix} \sim N\left(\begin{bmatrix} \bzero \\ \bzero \end{bmatrix},\, \sigma^2\begin{bmatrix} \bI_n & O \\ O & \bI_m \end{bmatrix}\right)\; .  
\end{align*}
  
  \item Easy to derive the conditional density:
\[
 p(\tilde{\by}\given \btheta, \by) =  p(\tilde{\by}\given \btheta) = N(\tilde{\by}\given \tilde{\bX}\bbeta, \sigma^2 \bI_m)
\]

\item Posterior predictive density:
\[
 p(\tilde{\by}\given \by) = \int N(\tilde{\by}\given \tilde{\bX}\bbeta, \sigma^2 \bI_m) p(\bbeta, \sigma^2\given \by) \mbox{d}\bbeta \mbox{d}\sigma^2\; .
\]

\item For each $i=1,2,\ldots,N$ do the following: 
 \begin{enumerate}\setlength{\itemsep}{0.25cm}
  \item Draw $\displaystyle \{\bbeta_{(i)},\sigma^2_{(i)}\} \sim p(\bbeta,\sigma^2\given \by)$
  \item Draw $\displaystyle \tilde{\by}_{(i)} \sim N(\tilde{\bX}\bbeta_{(i)}, \sigma^2_{(i)} \bI_m)$
 \end{enumerate}
 \end{itemize}

\end{frame}

\begin{frame}{Bayesian predictions from general linear regression}
 
 \begin{itemize}\setlength{\itemsep}{0.cm}  
  \item For example, consider the case with general $\bV_y$. Specify:

  \begin{align*}
 \begin{bmatrix} \by \\ \tilde{\by} \end{bmatrix} = \begin{bmatrix} \bX \\ \tilde{\bX} \end{bmatrix}\bbeta + \begin{bmatrix} \bepsilon \\ \tilde{\bepsilon} \end{bmatrix}\;;\quad \begin{bmatrix} \bepsilon \\ \tilde{\bepsilon} \end{bmatrix} \sim N\left(\begin{bmatrix} \bzero \\ \bzero \end{bmatrix},\, \sigma^2\begin{bmatrix} \bV_{y} & \bV_{y\tilde{y}} \\ \bV_{y\tilde{y}}^{\top} & \bV_{\tilde{y}} \end{bmatrix}\right)\; .  
\end{align*}
  
  \item Derive the conditional density $\displaystyle p(\tilde{\by}\given \btheta, \by) = N\left(\tilde{\by}\given \bmu_{\tilde{y}|y}, \sigma^2 \bV_{\tilde{y}|y}\right)$:
% \begin{align*}
%  p(\tilde{y}\given \theta, y) = N\left(\tilde{y}\given \tilde{X}\beta + V_{y\tilde{y}}^{\top}V_y^{-1}(y-X\beta), \sigma^2 \left{V_{\tilde{y}} -  V_{y\tilde{y}}^{\top}V_y^{-1}V_{y\tilde{y}} \right}\right)
% \end{align*}
\begin{align*}
% p(\tilde{y}\given \theta, y) &= N\left(\tilde{y}\given \mu_{\tilde{y}|y}, \sigma^2 V_{\tilde{y}|y}\right) \\
 \bmu_{\tilde{y}|y} &= \tilde{\bX}\bbeta + \bV_{y\tilde{y}}^{\top}\bV_y^{-1}(\by-\bX\bbeta)\;;\quad \bV_{\tilde{y}|y} = \bV_{\tilde{y}} -  \bV_{y\tilde{y}}^{\top}\bV_y^{-1}\bV_{y\tilde{y}}\;.
\end{align*}

\item Posterior predictive density:
\[
 p(\tilde{\by}\given \by) = \int N\left(\tilde{\by}\given \bmu_{\tilde{y}|y}, \sigma^2 \bV_{\tilde{y}|y}\right) p(\bbeta, \sigma^2\given \by) \mbox{d}\bbeta \mbox{d}\sigma^2\; .
\]

\item For each $i=1,2,\ldots,N$ do the following: 
 \begin{enumerate}\setlength{\itemsep}{0.25cm}
  \item Draw $\displaystyle \{\bbeta_{(i)},\sigma^2_{(i)}\} \sim p(\bbeta,\sigma^2\given \by)$
  \item Compute $\bmu_{\tilde{y}|y}$ using $\bbeta_{(i)}$ and draw $\displaystyle \tilde{\by}_{(i)} \sim N(\bmu_{\tilde{y}|y}, \sigma^2_{(i)} \bV_{\tilde{y}})$
 \end{enumerate}
 \end{itemize}

\end{frame}


\begin{frame}{Application to Bayesian Geostatistics}
 
 \begin{itemize}\setlength{\itemsep}{0.1cm}  
  \item Consider the spatial regression model
  \begin{align*}
   y(s_i) = \bx^{\top}(\bs_i)\bbeta + w(\bs_i) + \epsilon(\bs_i),
  \end{align*}
  where $w(\bs_i)$'s are spatial random effects and $\epsilon(\bs_i)$'s are unstructured errors (``white noise'').
  
  \item $\bw = (w(\bs_1), w(\bs_2), \ldots, w(\bs_n))^{\top} \sim N(\bzero, \sigma^2 \bR(\phi))$
  
  \item $\bepsilon = (\epsilon(\bs_1), \epsilon(\bs_2), \ldots, \epsilon(\bs_n))^{\top} \sim N(\bzero, \tau^2 \bI_n)$
  
  \item Integrating out random effects leads to a Bayesian model:
  \[
    IG(\sigma^2\given a, b) \times N(\bbeta \given \bmu_{\beta}, \sigma^2\bV_{\beta}) \times N(\by\given \bX\bbeta, \sigma^2 \bV_y)
  \]
  where $\bV_{y} = \bR(\phi) + \alpha \bI_n$ and $\alpha = \tau^2/\sigma^2$\; .
  
  \item Fixing $\phi$ and $\alpha$ (e.g., from variogram or other EDA) yields a conjugate Bayesian model.
  
  \item Exact posterior sampling is easily achieved as before!
 \end{itemize}

\end{frame}

\begin{frame}{Inference on spatial random effects}
 
 \begin{itemize}\setlength{\itemsep}{0.4cm}
  \item Rewrite the model in terms of $\bw$ as:
 \begin{align*}
   IG(\sigma^2\given a, b) \times & N(\bbeta \given \bmu_{\beta}, \sigma^2\bV_{\beta}) \times N(\bw\given \bzero, \sigma^2 \bR(\phi)) \\
    &\times N(\by\given \bX\bbeta+\bw, \tau^2\bI_n)\; .
 \end{align*}

  \item Posterior distribution of spatial random effects $\bw$:
  \[
   p(\bw\given \by) = \int N(\bw\given \bM\bm,\sigma^2\bM)\times p(\bbeta,\sigma^2\given \by) \mbox{d}\bbeta\mbox{d}\sigma^2\;,
  \]
where $\displaystyle \bm = (1/\alpha)(\by-\bX\bbeta)$ and $\displaystyle \bM^{-1} = \bR^{-1}(\phi) + (1/\alpha)\bI_n$.

  \item For each $i=1,2,\ldots,N$ do the following: 
 \begin{enumerate}\setlength{\itemsep}{0.25cm}
  \item Draw $\displaystyle \{\bbeta_{(i)},\sigma^2_{(i)}\} \sim p(\bbeta,\sigma^2\given \by)$
  \item Compute $\bm$ from $\bbeta_{(i)}$ and draw $\displaystyle \bw_{(i)} \sim N(\bM\bm, \sigma^2_{(i)}\bM)$
 \end{enumerate}
 \end{itemize}
 
\end{frame}


\begin{frame}{Inference on the process}
 
 \begin{itemize}\setlength{\itemsep}{0.4cm}
  \item Posterior distribution of $w(\bs_0)$ at new location $\bs_0$:
  \[
   p(w(\bs_0)\given \by) = \int N(w(\bs_0)\given \mu_{w(\bs_0)|w}, \sigma^2_{w(\bs_0)|w})\times p(\sigma^2, \bw \given \by) \mbox{d}\sigma^2\mbox{d}\bw\;,
  \]
  where 
  \begin{align*}
    \mu_{w(\bs_0)|w} &= \br^{\top}(\bs_0; \phi)\bR^{-1}(\phi)\bw\;; \\
    \sigma^2_{w(\bs_0)|w} &=  \sigma^2\{1-\br^{\top}(\bs_0; \phi)\bR^{-1}(\phi)\br(\bs_0,\phi)\}
  \end{align*}
  \item For each $i=1,2,\ldots,N$ do the following: 
 \begin{enumerate}\setlength{\itemsep}{0.25cm}
  \item Compute $ \mu_{w(\bs_0)|w}$ and $\sigma^2_{w(\bs_0)|w}$ from $\bw_{(i)}$ and $\sigma^2_{(i)}$. 
  \item Draw $\displaystyle w_{(i)}(\bs_0) \sim N(\mu_{w(\bs_0)|w}, \sigma^2_{w(\bs_0)|w})$.
 \end{enumerate}
 \end{itemize}
 
\end{frame}

\begin{frame}{Bayesian ``kriging'' or prediction}
 
 \begin{itemize}\setlength{\itemsep}{0.4cm}
  \item Posterior predictive distribution at new location $\bs_0$ is $p(y(\bs_0)\given \by)$:
  \[
   \int N(y(\bs_0)\given \bx^{\top}(s_0)\bbeta + w(\bs_0), \alpha \sigma^2)\times p(\bbeta, \sigma^2, \bw \given \by)\mbox{d}\bbeta \mbox{d}\sigma^2\mbox{d}\bw\;,
  \]
  \item For each $i=1,2,\ldots,N$ do the following: 
 \begin{enumerate}\setlength{\itemsep}{0.25cm}
%  \item Compute $ \mu_{w(s_0)|w}$ and $\sigma^2_{w(s_0)|w}$ from $w_{(i)}$ and $\sigma^2_{(i)}$. 
  \item Draw $\displaystyle y_{(i)}(\bs_0) \sim N(\bx^{\top}(\bs_0)\bbeta_{(i)} + w_{(i)}(s_0), \alpha\sigma^2_{(i)})$.
 \end{enumerate}
 \end{itemize}
 
\end{frame}


\begin{frame}{Non-conjugate models: The Gibbs Sampler}

{\small

\begin{itemize}\setlength{\itemsep}{0.4cm} 
\item Let $\theta=(\theta_1,\ldots,\theta_p)$ be the parameters in our model.

\item Initialize with starting values $\theta^{(0)}=(\theta_{1}^{(0)},\ldots,\theta_{p}^{(0)})$ 

\item For $j=1,\ldots,N$, update successively using the \emph{full conditional} distributions:
\begin{description}\setlength{\itemsep}{0.1cm} 
\item $\theta_{1}^{(j)} \sim
p(\theta_{1}^{(j)}\given\theta_{2}^{(j-1)},\ldots,\theta_{p}^{(j-1)},\by)$

\item $\theta_{2}^{(j)} \sim
p(\theta_{2}\given\theta_{1}^{(j)},\theta_{3}^{(j-1)},\ldots,\theta_{p}^{(j-1)}, \by)$

\item $\vdots$

\item (the generic $k^{th}$ element) $\theta_{k}^{(j)} \sim
p(\theta_{k}|\theta_{1}^{(j)},\ldots,\theta_{k-1}^{(j)},\theta_{k+1}^{(j-1)},\ldots,\theta_{p}^{(j-1)},\by)$

\item $\vdots$

\item $\theta_{p}^{(j)} \sim
p(\theta_{p}\given\theta_{1}^{(j)},\ldots,\theta_{p-1}^{(j)}, \by)$
\end{description}
\end{itemize}
}
\end{frame}

% \begin{frame}
%  
% \begin{itemize}\setlength{\itemsep}{0.5cm}
%  \item Example: Consider the linear model. Suppose we set $p(\sigma^2)=IG(\sigma^2\given a,b)$ and $p(\bbeta) \propto 1$.
% 
%  \item The full conditional distributions are:
% \begin{align*}
% p(\bbeta\given\by,\sigma^2) &= N(\bbeta\given(X^{T}X)^{-1}X^{T}\by,\sigma^2(X^{T}X)^{-1})\\
% p(\sigma^2\given \by, \bbeta) &= IG\left(\sigma^2\given a+n/2,b+\frac{1}{2}(\by-X\bbeta)^{T}(\by - X\bbeta)\right).
% \end{align*}
% 
% \item Thus, the Gibbs sampler will initialize $(\bbeta^{(0)},\sigma^{2(0)})$ and draw, for $j=1,\ldots, M$:
% \begin{itemize}
% \item Draw $\bbeta^{(j)} \sim N((X^{T}X)^{-1}X^{T}\by,\sigma^{2(j-1)}(X^{T}X)^{-1})$
% \item Draw $\sigma^{2(j)} \sim IG\left(a+n/2,b+\frac{1}{2}(\by-X\bbeta^{(j)})^{T}(\by - X\bbeta^{(j)})\right)$
% \end{itemize}
% 
% \end{itemize}
% 
% \end{frame}

\begin{frame}

\begin{itemize}\setlength{\itemsep}{0.5cm}
\item In principle, the Gibbs sampler will work for extremely
complex hierarchical models. The only issue is sampling from the
full conditionals. They may not be amenable to easy sampling --
when these are not in closed form. A more general and extremely
powerful - and often easier to code - algorithm is the
Metropolis-Hastings (MH) algorithm.

\item This algorithm also constructs a Markov chain, but does not necessarily care about full conditionals.

\item Popular approach: Embed Metropolis steps within Gibbs to draw from full conditionals that are not accessible to directly generate from. 
\end{itemize}

\end{frame}

{\footnotesize
\begin{frame}{The Metropolis-Hastings Algorithm}

 \begin{itemize}\setlength{\itemsep}{0.cm}
  \item The Metropolis-Hastings algorithm: Start with a initial value for $\theta=\theta^{(0)}$. Select a \emph{candidate} or \emph{proposal} distribution from which to propose a value of $\theta$ at the $j$-th iteration: $\theta^{(j)} \sim q(\theta^{(j-1)}, \nu)$. For example, $q(\theta^{(j-1)},\nu)=N(\theta^{(j-1)},\nu)$ with $\nu$ fixed.

\item Compute
\[
r =
\frac{p(\theta^{\ast}\given y)q(\theta^{(j-1)}\,|\,\theta^{\ast},\nu)}{p(\theta^{(j-1)}\given y)q(\theta^{\ast}\,|\,\theta^{(j-1)}\nu)}
\]

\item If $r \geq 1$ then set $\theta^{(j)}=\theta^{\ast}$. If $r \leq 1$ then draw $U\sim(0,1)$. If $U \leq r$ then
$\theta^{(j)}=\theta^{\ast}$. Otherwise, $\theta^{(j)}=\theta^{(j-1)}$.

\item Repeat for $j=1,\ldots N$. This yields $\theta^{(1)},\ldots,\theta^{(N)}$, which, after a burn-in period, will be samples from the true posterior distribution. It is important to monitor the acceptance ratio $r$ of the sampler through the iterations. Rough recommendations: for vector updates $r\approx 20\%$., for scalar updates $r\approx 40\%$. This can be controlled by ``tuning'' $\nu$.

\item Popular approach: Embed Metropolis steps within Gibbs to draw from full conditionals that are not accessible to directly generate from. 

\end{itemize}

\end{frame}
}

{\scriptsize
\begin{frame}
\begin{itemize}\setlength{\itemsep}{0cm}

\item Example: For the linear model, our parameters are $(\beta,\sigma^2)$. We write $\theta=(\beta,\log(\sigma^2))$ and, at the $j$-th iteration, propose $\theta^{\ast}\sim N(\theta^{(j-1)},\Sigma)$. The log transformation on $\sigma^2$ ensures that all components of $\theta$ have support on the entire real line and can have meaningful proposed values from the multivariate normal. But we need to transform our prior to $p(\beta,\log(\sigma^2))$.

\item Let $z=\log(\sigma^2)$ and assume $p(\beta,z)=p(\beta)p(z)$. Let us derive $p(z)$. \alert{REMEMBER:} we need to adjust for the jacobian.  Then $p(z)=p(\sigma^2)|d\sigma^2/dz| = p(e^z)e^z$. The jacobian here is $e^z=\sigma^2$.

\item Let $p(\beta)=1$ and an $p(\sigma^2)=IG(\sigma^2\given a,b)$. Then log-posterior is:
\[
-(a+n/2+1)z + z -
\frac{1}{e^z}\{b+\frac{1}{2}(Y-X\beta)^{T}(Y-X\beta)\}.
\]

\item A symmetric proposal distribution, say $q(\theta^*|\theta^{(j-1)},\Sigma)= N(\theta^{(j-1)},\Sigma)$, cancels out in $r$.  In practice it is better to compute $\log(r)$:  $\log(r)=\log(p(\theta^{\ast}\given y) - \log(p(\theta^{(j-1)}\given y))$. For the proposal, $N(\theta^{(j-1)},\Sigma)$, $\Sigma$ is a $d\times d$ variance-covariance matrix, and $d=\dim(\theta) = p+1$. 

\item If $\log r \geq 0$ then set $\theta^{(j)}=\theta^{\ast}$. If $\log r \leq 0$ then draw $U\sim(0,1)$. If $U \leq r$ (or $\log U \leq \log r$) then $\theta^{(j)}=\theta^{\ast}$. Otherwise, $\theta^{(j)}=\theta^{(j-1)}$.

\item Repeat the above procedure for $j=1,\ldots M$ to obtain samples $\theta^{(1)},\ldots,\theta^{(M)}$.

\end{itemize}

\end{frame}
}


\end{document}
