%\documentclass[hyperref={pdfpagelabels=false},compress]{beamer}

% \titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.pdf}}

\input{frontStuff.tex}

\title[]{Bayesian Linear Models}
\input{authors.tex}

\begin{document}

\maketitle


\begin{frame}{Linear Regression}

\begin{itemize}\setlength{\itemsep}{0.5cm}
\item Linear regression is, perhaps, \emph{the} most widely used statistical modeling tool.

\item It addresses the following question: How does a quantity of primary interest, $y$, vary as (depend upon) another quantity, or set of quantities, $x$?  

\item The quantity $y$ is called the \emph{response} or \emph{outcome variable}. Some people simply refer to it as the \emph{dependent variable}.

\item The variable(s) $x$ are called \emph{explanatory variables}, \emph{covariates} or simply \emph{independent variables}.

\item In general, we are interested in the conditional distribution of $y$, given $x$, parametrized as $p(y\given \theta, x)$.
\end{itemize}

\end{frame}

\begin{frame}
 
\begin{itemize}\setlength{\itemsep}{0.4cm}
 \item Typically, we have a set of \emph{units} or \emph{experimental subjects} $i=1,2,\ldots,n$.

 \item For each of these units we have measured an outcome $y_i$ and a set of explanatory variables $x_i^{\top} = (1, x_{i1}, x_{i2}, \ldots, x_{ip})$. 

 \item The first element of $x_i^{\top}$ is often taken as $1$ to signify the presence of an ``intercept''. 
 
 \item We collect the outcome and explanatory variables into an $n\times 1$ vector and an $n\times (p+1)$ matrix:
\begin{align*}
 y = \begin{pmatrix} 
	y_1 \\
        y_2 \\
	\vdots \\
	y_n
       \end{pmatrix};\quad X = \begin{bmatrix}
			1 & x_{11} & x_{12} & \ldots & x_{1p} \\
			1 & x_{21} & x_{22} & \ldots & x_{2p} \\
			\vdots & \vdots & \vdots & \vdots & \vdots \\
			1 & x_{n1} & x_{n2} & \ldots & x_{np}
                       \end{bmatrix} = \begin{pmatrix} 
	x_1^{\top} \\
        x_2^{\top} \\
	\vdots \\
	x_n^{\top}
       \end{pmatrix}\; . 
\end{align*}

\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}
 \item The linear model is the most fundamental of all serious statistical models underpinning:
\vspace*{0.5cm}
	\begin{itemize}\setlength{\itemsep}{0.4cm}
	\item ANOVA: $y_i$ is continuous, $x_{ij}$'s are \emph{all} categorical
	\item REGRESSION: $y_i$ is continuous, $x_{ij}$'s are continuous
	\item ANCOVA: $y_i$ is continuous, $x_{ij}$'s are continuous for some $j$ and categorical for others. 
	\end{itemize}
\end{itemize} 
\end{frame}

\begin{frame}{Conjugate Bayesian Linear Regression}
 
\begin{itemize}\setlength{\itemsep}{0.4cm}
 \item \vskip-2mm A conjugate Bayesian linear model is given by:
\begin{align*}
& y_i\given \mu_i,\sigma^2, X \stackrel{ind}{\sim} N(\mu_i,\sigma^2);\quad i=1,2,\ldots,n\;;\\
& \mu_i = \beta_0 + \beta_1x_{i1} + \cdots + \beta_{p} x_{ip} = x_i^{\top}\beta\;;\quad \beta = (\beta_0,\beta_1,\ldots,\beta_{p})^{\top}\;; \\
& \beta \given \sigma^2 \sim N(\mu_{\beta}, \sigma^2 V_{\beta})\;;\quad  \sigma^2 \sim IG(a,b)\; .
\end{align*}

\item Unknown parameters include the regression parameters and the variance, i.e. $\theta = \{\beta, \sigma^2\}$. 

\item We assume $X$ is observed without error and all inference is conditional on $X$.

\item The above model is often written it terms of the posterior density $p(\theta \given y) \propto p(\theta, y)$:
\begin{align*}
 IG(\sigma^2\given a,b) \times N(\beta \given \mu_{\beta}, \sigma^2 V_{\beta}) \times \prod_{i=1}^n N(y_i \given x_i^{\top}\beta, \sigma^2)\;.
\end{align*}

\end{itemize}

\end{frame}


\begin{frame}{Conjugate Bayesian (General) Linear Regression}
 
\begin{itemize}\setlength{\itemsep}{0.1cm}
 \item \vskip-2mm A more general conjugate Bayesian linear model is given by:
\begin{align*}
& y \given \beta,\sigma^2, X \sim N(X\beta, \sigma^2 V_y)\\
& \beta \given \sigma^2 \sim N(\mu_{\beta}, \sigma^2 V_{\beta})\;;\\
& \sigma^2 \sim IG(a,b)\; .
\end{align*}

\item $V_y$, $V_{\beta}$ and $\mu_{\beta}$ are assumed fixed.

\item Unknown parameters include the regression parameters and the variance, i.e. $\theta = \{\beta, \sigma^2\}$. 

\item We assume $X$ is observed without error and all inference is conditional on $X$.

\item The posterior density $p(\theta \given y) \propto p(\theta, y)$:
\begin{align*}
 IG(\sigma^2\given a,b) \times N(\beta \given \mu_{\beta}, \sigma^2 V_{\beta}) \times N(y \given X\beta, \sigma^2 V_{y})
\end{align*}

\item The model on the previous slide is a special case with $V_y = I_n$ ($n\times n$ identity matrix).
\end{itemize}

\end{frame}

\begin{frame}{Conjugate Bayesian (General) Linear Regression}
 
\begin{itemize}\setlength{\itemsep}{0.1cm}
 \item The joint posterior density can be written as
\begin{align*}
 p(\beta,\sigma^2 \given y) &\propto
 \begin{array}{ccc}
  \underbrace{IG(\sigma^2\given a^*, b^*)} & \times & \underbrace{N\left(\beta \given Mm, \sigma^2 M\right)}\\
  p(\sigma^2\given y) & & p(\beta\given \sigma^2, y)
 \end{array}\; , 
\end{align*}
where 
\begin{align*}
& a^* = a + \frac{n}{2}\;;\quad b^* = b + \frac{1}{2}\left(\mu_{\beta}^{\top}V_{\beta}^{-1}\mu_{\beta} + y^{\top}y - m^{\top}Mm\right)\;; \\ 
& m = V_{\beta}^{-1}\mu_{\beta} + X^{\top}V_y^{-1}y\;;\quad M^{-1} = V_{\beta}^{-1} + X^{\top} V_y^{-1} X\; .   
\end{align*}

\item Exact posterior sampling from $p(\beta, \sigma^2 \given y)$ will automatically yield samples from $p(\beta\given y)$ and $p(\sigma^2\given y)$.

\item For each $i=1,2,\ldots,N$ do the following: 
 \begin{enumerate}
  \item Draw $\displaystyle \sigma^2_{(i)} \sim IG(a^*,b^*)$
  \item Draw $\displaystyle \beta_{(i)} \sim N\left(Mm, \sigma^2_{(i)}M\right)$
 \end{enumerate}
 
 \item The above is sometimes referred to as \emph{composition sampling}.
\end{itemize}

\end{frame}

\begin{frame}{Exact sampling from joint posterior distributions}

\begin{itemize}\setlength{\itemsep}{0cm}
\item \vskip-4mm Suppose we wish to draw samples from a joint posterior:
\[
 p(\theta_1,\theta_2\given y) = p(\theta_1\given y) \times p(\theta_2\given \theta_1, y)\; .
\]

\item In conjugate models, it is often easy to draw samples from $p(\theta_1\given y)$ and from $p(\theta_2\given \theta_1,y)$. 

\item We can draw $M$ samples from $p(\theta_1,\theta_2\given y)$ as follows.

\item For each $i=1,2,\ldots,N$ do the following: 
 \begin{enumerate}\setlength{\itemsep}{0cm}
  \item Draw $\displaystyle \theta_{1(i)} \sim p(\theta_1\given y)$
  \item Draw $\displaystyle \theta_{2(i)} \sim p(\theta_2\given \theta_1,y)$
 \end{enumerate}

\item Remarkably, the $\theta_{2(i)}$'s drawn above have marginal distribution $p(\theta_2\given y)$ because: 
\vskip -9mm \begin{align*} P(\theta_2\leq u \given y) &= \mbox{E}_{\theta_2|y}\left[1(\theta_2\leq u)\right] = \mbox{E}_{\theta_1|y}\left\{\mbox{E}_{\theta_2|\theta_1, y}\left[1(\theta_2\leq u)\right]\right\} \\
  &\approx \frac{1}{N} \sum_{i=1}^N \mbox{E}_{\theta_2|\theta_{1(i)}, y}\left[1(\theta_2\leq u)\right] \approx  \frac{1}{N} \sum_{i=1}^N 1(\theta_{2(i)}\leq u)\; .
\end{align*}

\item ``Automatic Marginalization:'' We draw samples $p(\theta_1,\theta_2\given y)$ and automatically get samples from $p(\theta_1\given y)$ and $p(\theta_2\given y)$.

\end{itemize}

\end{frame}

\begin{frame}{Bayesian predictions from linear regression}
 
 \begin{itemize}\setlength{\itemsep}{0.4cm}
%   \item Consider first the Bayesian linear model with $V_y = I_n$:
%   \[
%    IG(\sigma^2\given a,b) \times N(\beta \given \mu_{\beta}, \sigma^2 V_{\beta}) \times N(y \given X\beta, \sigma^2I_n)\;.
%   \]
  \item Let $\tilde{y}$ denote an $m\times 1$ vector of outcomes we seek to predict based upon predictors $\tilde{X}$.
  \item We seek the posterior predictive density:
\[
 p(\tilde{y}\given y) = \int p(\tilde{y}\given \theta, y) p(\theta\given y) \mbox{d}\theta\; .
\]
  \item Posterior predictive inference: sample from $p(\tilde{y}\given y)$.
  \item For each $i=1,2,\ldots,N$ do the following: 
 \begin{enumerate}\setlength{\itemsep}{0.25cm}
  \item Draw $\displaystyle \theta_{(i)} \sim p(\theta\given y)$
  \item Draw $\displaystyle \tilde{y}_{(i)} \sim p(\tilde{y}\given \theta_{(i)},y)$
 \end{enumerate}
  
 \end{itemize}

\end{frame}

\begin{frame}{Bayesian predictions from linear regression (contd.)}
 
 \begin{itemize}\setlength{\itemsep}{0.cm}
  \item \vskip -2mm For legitimate probabilistic predictions (forecasting), the conditional distribution $p(\tilde{y}\given \theta, y)$ must be well-defined.
  
  \item For example, consider the case with $V_y = I_n$. Specify the linear model:
\begin{align*}
 \begin{bmatrix} y \\ \tilde{y} \end{bmatrix} = \begin{bmatrix} X \\ \tilde{X} \end{bmatrix}\beta + \begin{bmatrix} \epsilon \\ \tilde{\epsilon} \end{bmatrix}\;;\quad \begin{bmatrix} \epsilon \\ \tilde{\epsilon} \end{bmatrix} \sim N\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix},\, \sigma^2\begin{bmatrix} I_n & O \\ O & I_m \end{bmatrix}\right)\; .  
\end{align*}
  
  \item Easy to derive the conditional density:
\[
 p(\tilde{y}\given \theta, y) =  p(\tilde{y}\given \theta) = N(\tilde{y}\given \tilde{X}\beta, \sigma^2 I_m)
\]

\item Posterior predictive density:
\[
 p(\tilde{y}\given y) = \int N(\tilde{y}\given \tilde{X}\beta, \sigma^2 I_m) p(\beta, \sigma^2\given y) \mbox{d}\beta \mbox{d}\sigma^2\; .
\]

\item For each $i=1,2,\ldots,N$ do the following: 
 \begin{enumerate}\setlength{\itemsep}{0.25cm}
  \item Draw $\displaystyle \{\beta_{(i)},\sigma^2_{(i)}\} \sim p(\beta,\sigma^2\given y)$
  \item Draw $\displaystyle \tilde{y}_{(i)} \sim N(\tilde{X}\beta_{(i)}, \sigma^2_{(i)} I_m)$
 \end{enumerate}
 \end{itemize}

\end{frame}

\begin{frame}{Bayesian predictions from general linear regression}
 
 \begin{itemize}\setlength{\itemsep}{0.cm}  
  \item For example, consider the case with general $V_y$. Specify:

  \begin{align*}
 \begin{bmatrix} y \\ \tilde{y} \end{bmatrix} = \begin{bmatrix} X \\ \tilde{X} \end{bmatrix}\beta + \begin{bmatrix} \epsilon \\ \tilde{\epsilon} \end{bmatrix}\;;\quad \begin{bmatrix} \epsilon \\ \tilde{\epsilon} \end{bmatrix} \sim N\left(\begin{bmatrix} 0 \\ 0 \end{bmatrix},\, \sigma^2\begin{bmatrix} V_{y} & V_{y\tilde{y}} \\ V_{y\tilde{y}}^{\top} & V_{\tilde{y}} \end{bmatrix}\right)\; .  
\end{align*}
  
  \item Derive the conditional density $\displaystyle p(\tilde{y}\given \theta, y) = N\left(\tilde{y}\given \mu_{\tilde{y}|y}, \sigma^2 V_{\tilde{y}|y}\right)$:
% \begin{align*}
%  p(\tilde{y}\given \theta, y) = N\left(\tilde{y}\given \tilde{X}\beta + V_{y\tilde{y}}^{\top}V_y^{-1}(y-X\beta), \sigma^2 \left{V_{\tilde{y}} -  V_{y\tilde{y}}^{\top}V_y^{-1}V_{y\tilde{y}} \right}\right)
% \end{align*}
\begin{align*}
% p(\tilde{y}\given \theta, y) &= N\left(\tilde{y}\given \mu_{\tilde{y}|y}, \sigma^2 V_{\tilde{y}|y}\right) \\
 \mu_{\tilde{y}|y} &= \tilde{X}\beta + V_{y\tilde{y}}^{\top}V_y^{-1}(y-X\beta)\;;\quad V_{\tilde{y}|y} = V_{\tilde{y}} -  V_{y\tilde{y}}^{\top}V_y^{-1}V_{y\tilde{y}}\;.
\end{align*}

\item Posterior predictive density:
\[
 p(\tilde{y}\given y) = \int N\left(\tilde{y}\given \mu_{\tilde{y}|y}, \sigma^2 V_{\tilde{y}|y}\right) p(\beta, \sigma^2\given y) \mbox{d}\beta \mbox{d}\sigma^2\; .
\]

\item For each $i=1,2,\ldots,N$ do the following: 
 \begin{enumerate}\setlength{\itemsep}{0.25cm}
  \item Draw $\displaystyle \{\beta_{(i)},\sigma^2_{(i)}\} \sim p(\beta,\sigma^2\given y)$
  \item Compute $\mu_{\tilde{y}|y}$ using $\beta_{(i)}$ and draw $\displaystyle \tilde{y}_{(i)} \sim N(\mu_{\tilde{y}|y}, \sigma^2_{(i)} V_{\tilde{y}})$
 \end{enumerate}
 \end{itemize}

\end{frame}


\begin{frame}{Application to Bayesian Geostatistics}
 
 \begin{itemize}
  \item Consider the spatial regression model
  \begin{align*}
   y(s_i) = x^{\top}(s_i)\beta + w(s_i) + \epsilon(s_i)\; ,
  \end{align*}
  where $w(s_i)$'s are spatial random effects and $\epsilon(s_i)$'s are unstructured errors (``white noise'').
  
  \item $w = (w(s_1), w(s_2), \ldots, w(s_n))^{\top} \sim N(0, \sigma^2 R(\phi))$
  
  \item $\epsilon = (\epsilon(s_1), \epsilon(s_2), \ldots, \epsilon(s_n))^{\top} \sim N(0, \tau^2 I_n)$
  
  \item Integrating out random effects leads to a Bayesian model:
  \[
    IG(\sigma^2\given a, b) \times N(\beta \given \mu_{\beta}, \sigma^2V_{\beta}) \times N(y\given X\beta, \sigma^2 V_y)
  \]
  where $V_{y} = R(\phi) + \alpha I_n$ and $\alpha = \tau^2/\sigma^2$\; .
  
  \item Fixing $\phi$ and $\alpha$ (e.g., from variogram or other EDA) yields a conjugate Bayesian model.
  
  \item Exact posterior sampling is easily achieved as before.
 \end{itemize}

\end{frame}

\begin{frame}{Inference on spatial random effects}
 
 \begin{itemize}\setlength{\itemsep}{0.4cm}
  \item Rewrite the model in terms of $w$ as:
 \begin{align*}
   IG(\sigma^2\given a, b) \times & N(\beta \given \mu_{\beta}, \sigma^2V_{\beta}) \times N(w\given 0, \sigma^2 R(\phi)) \\
    &\times N(y\given X\beta+w, \tau^2I_n)\; .
 \end{align*}

  \item Posterior distribution of spatial random effects $w$:
  \[
   p(w\given y) = \int N(w\given Mm,\sigma^2M)\times p(\beta,\sigma^2\given y) \mbox{d}\beta\mbox{d}\sigma^2\;,
  \]
where $\displaystyle m = (1/\alpha)(y-X\beta)$ and $\displaystyle M^{-1} = R^{-1}(\phi) + (1/\alpha)I_n$.

  \item For each $i=1,2,\ldots,N$ do the following: 
 \begin{enumerate}\setlength{\itemsep}{0.25cm}
  \item Draw $\displaystyle \{\beta_{(i)},\sigma^2_{(i)}\} \sim p(\beta,\sigma^2\given y)$
  \item Compute $m$ from $\beta_{(i)}$ and draw $\displaystyle w_{(i)} \sim N(Mm, \sigma^2_{(i)}M)$
 \end{enumerate}
 \end{itemize}
 
\end{frame}


\begin{frame}{Inference on the process}
 
 \begin{itemize}\setlength{\itemsep}{0.4cm}
  \item Posterior distribution of $w(s_0)$ at new location $s_0$:
  \[
   p(w(s_0)\given y) = \int N(w(s_0)\given \mu_{w(s_0)|w}, \sigma^2_{w(s_0)|w})\times p(\sigma^2, w \given y) \mbox{d}\sigma^2\mbox{d}w\;,
  \]
  where 
  \begin{align*}
    \mu_{w(s_0)|w} &= r^{\top}(s_0; \phi)R^{-1}(\phi)w\;; \\
    \sigma^2_{w(s_0)|w} &=  \sigma^2\{1-r^{\top}(s_0; \phi)R^{-1}(\phi)r(s_0,\phi)\}
  \end{align*}
  \item For each $i=1,2,\ldots,N$ do the following: 
 \begin{enumerate}\setlength{\itemsep}{0.25cm}
  \item Compute $ \mu_{w(s_0)|w}$ and $\sigma^2_{w(s_0)|w}$ from $w_{(i)}$ and $\sigma^2_{(i)}$. 
  \item Draw $\displaystyle w_{(i)}(s_0) \sim N(\mu_{w(s_0)|w}, \sigma^2_{w(s_0)|w})$.
 \end{enumerate}
 \end{itemize}
 
\end{frame}

\begin{frame}{Bayesian ``kriging'' or prediction}
 
 \begin{itemize}\setlength{\itemsep}{0.4cm}
  \item Posterior predictive distribution at new location $s_0$ is $p(y(s_0)\given y)$:
  \[
   \int N(y(s_0)\given x^{\top}(s_0)\beta + w(s_0), \alpha \sigma^2)\times p(\beta, \sigma^2, w \given y)\mbox{d}\beta \mbox{d}\sigma^2\mbox{d}w\;,
  \]
  \item For each $i=1,2,\ldots,N$ do the following: 
 \begin{enumerate}\setlength{\itemsep}{0.25cm}
%  \item Compute $ \mu_{w(s_0)|w}$ and $\sigma^2_{w(s_0)|w}$ from $w_{(i)}$ and $\sigma^2_{(i)}$. 
  \item Draw $\displaystyle y_{(i)}(s_0) \sim N(x^{\top}(s_0)\beta_{(i)} + w_{(i)}(s_0), \alpha\sigma^2_{(i)})$.
 \end{enumerate}
 \end{itemize}
 
\end{frame}


\begin{frame}{Non-conjugate models: The Gibbs Sampler}

{\small

\begin{itemize}\setlength{\itemsep}{0.4cm} 
\item Let $\theta=(\theta_1,\ldots,\theta_p)$ be the parameters in our model.

\item $\theta^{(0)}=(\theta_{1}^{(0)},\ldots,\theta_{p}^{(0)})$ 

\item For $j=1,\ldots,M$, update successively using the \emph{full conditional} distributions:
\begin{description}
\item $\theta_{1}^{(j)} \sim
p(\theta_{1}^{(j)}\given\theta_{2}^{(j-1)},\ldots,\theta_{p}^{(j-1)},y)$

\item $\theta_{2}^{(j)} \sim
p(\theta_{2}\given\theta_{1}^{(j)},\theta_{3}^{(j-1)},\ldots,\theta_{p}^{(j-1)}, y)$

\item $\vdots$

\item (the generic $k^{th}$ element) $\theta_{k}^{(j)} \sim
p(\theta_{k}|\theta_{1}^{(j)},\ldots,\theta_{k-1}^{(j)},\theta_{k+1}^{(j-1)},\ldots,\theta_{p}^{(j-1)},y)$

\item $\vdots$

\item $\theta_{p}^{(j)} \sim
p(\theta_{p}\given\theta_{1}^{(j)},\ldots,\theta_{p-1}^{(j)}, y)$
\end{description}
\end{itemize}
}
\end{frame}

% \begin{frame}
%  
% \begin{itemize}\setlength{\itemsep}{0.5cm}
%  \item Example: Consider the linear model. Suppose we set $p(\sigma^2)=IG(\sigma^2\given a,b)$ and $p(\bbeta) \propto 1$.
% 
%  \item The full conditional distributions are:
% \begin{align*}
% p(\bbeta\given\by,\sigma^2) &= N(\bbeta\given(X^{T}X)^{-1}X^{T}\by,\sigma^2(X^{T}X)^{-1})\\
% p(\sigma^2\given \by, \bbeta) &= IG\left(\sigma^2\given a+n/2,b+\frac{1}{2}(\by-X\bbeta)^{T}(\by - X\bbeta)\right).
% \end{align*}
% 
% \item Thus, the Gibbs sampler will initialize $(\bbeta^{(0)},\sigma^{2(0)})$ and draw, for $j=1,\ldots, M$:
% \begin{itemize}
% \item Draw $\bbeta^{(j)} \sim N((X^{T}X)^{-1}X^{T}\by,\sigma^{2(j-1)}(X^{T}X)^{-1})$
% \item Draw $\sigma^{2(j)} \sim IG\left(a+n/2,b+\frac{1}{2}(\by-X\bbeta^{(j)})^{T}(\by - X\bbeta^{(j)})\right)$
% \end{itemize}
% 
% \end{itemize}
% 
% \end{frame}

\begin{frame}

\begin{itemize}\setlength{\itemsep}{0.5cm}
\item In principle, the Gibbs sampler will work for extremely
complex hierarchical models. The only issue is sampling from the
full conditionals. They may not be amenable to easy sampling --
when these are not in closed form. A more general and extremely
powerful - and often easier to code - algorithm is the
Metropolis-Hastings (MH) algorithm.

\item This algorithm also constructs a Markov Chain, but does not necessarily care about full conditionals.

\item Popular approach: Embed Metropolis steps within Gibbs to draw from full conditionals that are not accessible to directly generate from. 
\end{itemize}

\end{frame}

{\footnotesize
\begin{frame}{The Metropolis-Hastings Algorithm}

 \begin{itemize}\setlength{\itemsep}{0.cm}
  \item The Metropolis-Hastings algorithm: Start with a initial value for $\theta=\theta^{(0)}$. Select a \emph{candidate} or \emph{proposal} distribution from which to propose a value of $\theta$ at the $j$-th iteration: $\theta^{(j)} \sim q(\theta^{(j-1)}, \nu)$. For example, $q(\theta^{(j-1)},\nu)=N(\theta^{(j-1)},\nu)$ with $\nu$ fixed.

\item Compute
\[
r =
\frac{p(\theta^{\ast}\given y)q(\theta^{(j-1)}\,|\,\theta^{\ast},\nu)}{p(\theta^{(j-1)}\given y)q(\theta^{\ast}\,|\,\theta^{(j-1)}\nu)}
\]

\item If $r \geq 1$ then set $\theta^{(j)}=\theta^{\ast}$. If $r \leq 1$ then draw $U\sim(0,1)$. If $U \leq r$ then
$\theta^{(j)}=\theta^{\ast}$. Otherwise, $\theta^{(j)}=\theta^{(j-1)}$.

\item Repeat for $j=1,\ldots M$. This yields $\theta^{(1)},\ldots,\theta^{(M)}$, which, after a burn-in period, will be samples from the true posterior distribution. It is important to monitor the acceptance ratio $r$ of the sampler through the iterations. Rough recommendations: for vector updates $r\approx 20\%$., for scalar updates $r\approx 40\%$. This can be controlled by ``tuning'' $\nu$.

\item Popular approach: Embed Metropolis steps within Gibbs to draw from full conditionals that are not accessible to directly generate from. 

\end{itemize}

\end{frame}
}

{\scriptsize
\begin{frame}
\begin{itemize}\setlength{\itemsep}{0cm}

\item Example: For the linear model, our parameters are $(\beta,\sigma^2)$. We write $\theta=(\beta,\log(\sigma^2))$ and, at the $j$-th iteration, propose $\theta^{\ast}\sim N(\theta^{(j-1)},\Sigma)$. The log transformation on $\sigma^2$ ensures that all components of $\theta$ have support on the entire real line and can have meaningful proposed values from the multivariate normal. But we need to transform our prior to $p(\beta,\log(\sigma^2))$.

\item Let $z=\log(\sigma^2)$ and assume $p(\beta,z)=p(\beta)p(z)$. Let us derive $p(z)$. \alert{REMEMBER:} we need to adjust for the jacobian.  Then $p(z)=p(\sigma^2)|d\sigma^2/dz| = p(e^z)e^z$. The jacobian here is $e^z=\sigma^2$.

\item Let $p(\beta)=1$ and an $p(\sigma^2)=IG(\sigma^2\given a,b)$. Then log-posterior is:
\[
-(a+n/2+1)z + z -
\frac{1}{e^z}\{b+\frac{1}{2}(Y-X\beta)^{T}(Y-X\beta)\}.
\]

\item A symmetric proposal distribution, say $q(\theta^*|\theta^{(j-1)},\Sigma)= N(\theta^{(j-1)},\Sigma)$, cancels out in $r$.  In practice it is better to compute $\log(r)$:  $\log(r)=\log(p(\theta^{\ast}\given y) - \log(p(\theta^{(j-1)}\given y))$. For the proposal, $N(\theta^{(j-1)},\Sigma)$, $\Sigma$ is a $d\times d$ variance-covariance matrix, and $d=\dim(\theta) = p+1$. 

\item If $\log r \geq 0$ then set $\theta^{(j)}=\theta^{\ast}$. If $\log r \leq 0$ then draw $U\sim(0,1)$. If $U \leq r$ (or $\log U \leq \log r$) then $\theta^{(j)}=\theta^{\ast}$. Otherwise, $\theta^{(j)}=\theta^{(j-1)}$.

\item Repeat the above procedure for $j=1,\ldots M$ to obtain samples $\theta^{(1)},\ldots,\theta^{(M)}$.

\end{itemize}

\end{frame}
}


\end{document}
